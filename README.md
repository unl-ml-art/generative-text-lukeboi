# Project 1 Generative Text

Luke Farritor, yourcontact@unl.edu

## Abstract

One of my favorite corners of the internet is [Marginal Revoultion](https://marginalrevolution.com/) (MR), a blog by economists Tyler Cowen and Alex Tabarrok. The authors share a few posts a day covering a wide range of topics from economics, art, politics, philosophy and everything in between. MR has a comments section, which ranges from high quality discussion to low quality jokes and everything in between. I'd like to finetune GPT-2 to generate MR comments based on real posts. Creating a datset is easy - MR has had comments ranging back over 15 years, and scraping should not be difficult. As a strech goal, I could create a seperate lookalike/parody website showcasing the fake comments, with a disclaimer that the site is not affiliated with the real MR.

Include your abstract here. This should be one paragraph clearly describing your concept, method, and results. This should tell us what architecture/approach you used. Also describe your creative goals, and whether you were successful in achieving them. Also could describe future directions.

## Model/Data

Briefly describe the files that are included with your repository:
- trained models
- training data (or link to training data). what is your corpus?

## Code

Your code for generating your project:
- training_code.py or training_code.ipynb - your training code
- generative_code.py or generative_code.ipynb - your generation code

## Results

- Documentation of your generative text in an effective form. A file with your generated text (.pdf, .doc, .txt). 

## Technical Notes

Any implementation details or notes we need to repeat your work. 
- Does this code require other pip packages, software, etc?
- Does it run on some other (non-datahub) platform? (CoLab, etc.)

## Reference

References to any papers, techniques, repositories you used:
- Papers
  - [This is a paper](this_is_the_link.pdf)
- Repositories
- Blog posts
