{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ba83ba-4c15-4e3b-9702-9c89b3e3fe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpt-2-simple in /home/emar349/lukethecdr/.local/lib/python3.9/site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from gpt-2-simple) (1.19.5)\n",
      "Requirement already satisfied: regex in /home/emar349/lukethecdr/.local/lib/python3.9/site-packages (from gpt-2-simple) (2022.1.18)\n",
      "Requirement already satisfied: tensorflow>=2.5.1 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from gpt-2-simple) (2.6.0)\n",
      "Requirement already satisfied: requests in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from gpt-2-simple) (2.25.1)\n",
      "Requirement already satisfied: toposort in /home/emar349/lukethecdr/.local/lib/python3.9/site-packages (from gpt-2-simple) (1.7)\n",
      "Requirement already satisfied: tqdm in /home/emar349/lukethecdr/.local/lib/python3.9/site-packages (from gpt-2-simple) (4.62.3)\n",
      "Requirement already satisfied: keras~=2.6 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.6.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.15.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.39 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.39.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.16.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.37.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.12)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.7.4.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.1.2)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.7.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from requests->gpt-2-simple) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from requests->gpt-2-simple) (1.26.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from requests->gpt-2-simple) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from requests->gpt-2-simple) (2021.10.8)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (0.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (2.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (59.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.1->gpt-2-simple) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gpt-2-simple --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c0ffb0-ca37-400f-912d-ada1755793c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "model_name = \"124M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a55d4d-e1a7-49f0-a636-db9f68498291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 727Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 4.20Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 1.02Git/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:08, 59.4Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 618Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 5.48Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 5.40Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad0e432-a15e-4026-9cd9-827c3ecba7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 03:13:41.111067: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-22 03:13:41.778369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 28448 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n",
      "2022-02-22 03:13:44.665907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 28448 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 56332 tokens\n",
      "Training...\n",
      "[1 | 5.99] loss=3.69 avg=3.69\n",
      "[2 | 6.75] loss=3.65 avg=3.67\n",
      "[3 | 7.52] loss=3.63 avg=3.65\n",
      "[4 | 8.29] loss=3.57 avg=3.63\n",
      "[5 | 9.12] loss=3.64 avg=3.63\n",
      "[6 | 9.89] loss=3.54 avg=3.62\n",
      "[7 | 10.66] loss=3.60 avg=3.62\n",
      "[8 | 11.43] loss=3.53 avg=3.61\n",
      "[9 | 12.20] loss=3.43 avg=3.58\n",
      "[10 | 12.97] loss=3.36 avg=3.56\n",
      "[11 | 13.74] loss=3.32 avg=3.54\n",
      "[12 | 14.51] loss=3.50 avg=3.53\n",
      "[13 | 15.28] loss=3.40 avg=3.52\n",
      "[14 | 16.05] loss=3.55 avg=3.53\n",
      "[15 | 16.82] loss=3.16 avg=3.50\n",
      "[16 | 17.59] loss=3.45 avg=3.50\n",
      "[17 | 18.35] loss=3.06 avg=3.47\n",
      "[18 | 19.12] loss=3.26 avg=3.46\n",
      "[19 | 19.89] loss=3.19 avg=3.44\n",
      "[20 | 20.66] loss=3.23 avg=3.43\n",
      "[21 | 21.43] loss=3.31 avg=3.42\n",
      "[22 | 22.20] loss=3.14 avg=3.41\n",
      "[23 | 23.03] loss=3.23 avg=3.40\n",
      "[24 | 23.80] loss=2.93 avg=3.38\n",
      "[25 | 24.57] loss=3.20 avg=3.37\n",
      "[26 | 25.34] loss=2.96 avg=3.35\n",
      "[27 | 26.10] loss=2.76 avg=3.33\n",
      "[28 | 26.87] loss=2.95 avg=3.31\n",
      "[29 | 27.64] loss=3.06 avg=3.30\n",
      "[30 | 28.41] loss=2.85 avg=3.28\n",
      "[31 | 29.18] loss=2.79 avg=3.27\n",
      "[32 | 29.95] loss=2.94 avg=3.25\n",
      "[33 | 30.72] loss=3.00 avg=3.24\n",
      "[34 | 31.49] loss=2.96 avg=3.24\n",
      "[35 | 32.26] loss=2.50 avg=3.21\n",
      "[36 | 33.03] loss=2.66 avg=3.19\n",
      "[37 | 33.80] loss=2.70 avg=3.18\n",
      "[38 | 34.57] loss=3.08 avg=3.17\n",
      "[39 | 35.33] loss=2.94 avg=3.17\n",
      "[40 | 36.10] loss=2.43 avg=3.14\n",
      "[41 | 36.87] loss=2.78 avg=3.13\n",
      "[42 | 37.64] loss=2.64 avg=3.12\n",
      "[43 | 38.41] loss=2.38 avg=3.10\n",
      "[44 | 39.18] loss=2.62 avg=3.08\n",
      "[45 | 39.95] loss=2.43 avg=3.07\n",
      "[46 | 40.72] loss=2.71 avg=3.06\n",
      "[47 | 41.48] loss=2.68 avg=3.05\n",
      "[48 | 42.25] loss=2.15 avg=3.02\n",
      "[49 | 43.02] loss=2.45 avg=3.01\n",
      "[50 | 43.79] loss=2.32 avg=2.99\n",
      "[51 | 44.56] loss=2.29 avg=2.97\n",
      "[52 | 45.33] loss=2.41 avg=2.96\n",
      "[53 | 46.09] loss=2.28 avg=2.94\n",
      "[54 | 46.86] loss=2.31 avg=2.93\n",
      "[55 | 47.63] loss=2.15 avg=2.91\n",
      "[56 | 48.40] loss=2.05 avg=2.89\n",
      "[57 | 49.17] loss=2.38 avg=2.88\n",
      "[58 | 49.94] loss=2.22 avg=2.86\n",
      "[59 | 50.71] loss=2.27 avg=2.85\n",
      "[60 | 51.48] loss=2.25 avg=2.84\n",
      "[61 | 52.25] loss=2.27 avg=2.82\n",
      "[62 | 53.05] loss=2.12 avg=2.81\n",
      "[63 | 53.82] loss=1.81 avg=2.79\n",
      "[64 | 54.58] loss=2.17 avg=2.77\n",
      "[65 | 55.35] loss=2.24 avg=2.76\n",
      "[66 | 56.12] loss=2.17 avg=2.75\n",
      "[67 | 56.89] loss=1.91 avg=2.73\n",
      "[68 | 57.66] loss=1.82 avg=2.72\n",
      "[69 | 58.43] loss=1.85 avg=2.70\n",
      "[70 | 59.20] loss=1.90 avg=2.68\n",
      "[71 | 59.96] loss=1.90 avg=2.67\n",
      "[72 | 60.73] loss=1.44 avg=2.64\n",
      "[73 | 61.50] loss=1.51 avg=2.62\n",
      "[74 | 62.27] loss=1.88 avg=2.61\n",
      "[75 | 63.04] loss=1.87 avg=2.59\n",
      "[76 | 63.81] loss=1.81 avg=2.58\n",
      "[77 | 64.58] loss=1.68 avg=2.56\n",
      "[78 | 65.35] loss=1.58 avg=2.54\n",
      "[79 | 66.11] loss=2.03 avg=2.53\n",
      "[80 | 66.88] loss=1.70 avg=2.52\n",
      "[81 | 67.65] loss=1.94 avg=2.51\n",
      "[82 | 68.42] loss=1.70 avg=2.50\n",
      "[83 | 69.19] loss=1.79 avg=2.48\n",
      "[84 | 69.96] loss=1.54 avg=2.47\n",
      "[85 | 70.73] loss=1.51 avg=2.45\n",
      "[86 | 71.50] loss=1.50 avg=2.43\n",
      "[87 | 72.26] loss=1.06 avg=2.41\n",
      "[88 | 73.03] loss=1.21 avg=2.39\n",
      "[89 | 73.80] loss=1.67 avg=2.38\n",
      "[90 | 74.57] loss=1.43 avg=2.36\n",
      "[91 | 75.34] loss=1.32 avg=2.34\n",
      "[92 | 76.11] loss=1.17 avg=2.32\n",
      "[93 | 76.88] loss=1.57 avg=2.31\n",
      "[94 | 77.64] loss=0.93 avg=2.29\n",
      "[95 | 78.41] loss=0.96 avg=2.27\n",
      "[96 | 79.18] loss=1.25 avg=2.25\n",
      "[97 | 79.95] loss=1.16 avg=2.23\n",
      "[98 | 80.72] loss=1.15 avg=2.22\n",
      "[99 | 81.49] loss=1.21 avg=2.20\n",
      "[100 | 82.26] loss=1.29 avg=2.19\n",
      "======== SAMPLE 1 ========\n",
      " lower, and this has negative consequences for downstream health. For instance, the higher the salt content, the lower the risk of infection relative to plant and animal sources, as well as the slower the water loss, resulting in slower evaporation. Regarding mangrove scrub, he seems to have focused on the things that had less to do with him and agriculture: invertebrates; invertebrates; invertebrates; invertebrates; invertebrates; invertebrates; invertebrates. Given the relatively low density of the bacteria in the water, invertebrates and mangrove scrub were probably not worth the risk of infection.\n",
      "A new comment:\n",
      "Is this the first time Amazon forest fires have killed more people?\n",
      "A new comment:\n",
      "My favorite sentence was 'the deaths caused largely economic and social damage'\n",
      "A new comment:\n",
      "\"And yet...\" I believe this is a very good conversation starter. But the thing about economics -- and liberal psychology -- is you get started talking, and you move on to the next part -- perhaps the best part -- and you find another way to talk -- by moving beyond \"the deaths caused mostly economic and social damage\" -- and instead moving onto \"how people can live longer, in a more prosperous world without killing each other\". --and then you start pivoting your thinking towards \"how people could live better, more happily without doing so\" -- and you end up with a conversation starter that is really just trying to decide between two options: 1) It is the long term beneficial to avoid doing harm to each other that is beneficial for the long term. 2) It is the long term beneficial to avoid doing harm to any other person. 3) It is the long term detrimental to society overall to do so. And so on. -- Ezra Klein\n",
      "A new comment:\n",
      "I wonder if the reason so many people have reverted to Thomassonism is because they see a rhetorical error vs. a moralism - both of which are important in making moral arguments vs. inducing one in the first place\n",
      "A new comment:\n",
      "I believe this is a very good conversation starter. The reason why liberalism tends to exhaust itself with long lists of ills is because it completely forgotten about the other ills. So ironically, the reason why liberals have reverted to anti-crisis economics is because of this. The problem with anti-crisis economics is that it focuses heavily on losses rather than gains. Instead of tracking losses as losses (which it does at least now), the Macroeconomists just add in losses (which they did at the beginning) and then work out what those losses are. When they're able to correct for misallocations of losses (which they did at the beginning), the momentum will then be shifted to the consumption sectors where wearer (cost-benefit analysis) will find more efficient ways of measuring losses.\n",
      "A new comment:\n",
      "\"The Macroeconomists\" Ezra\n",
      "A new comment:\n",
      "The one that makes my day. The one that makes my job easier. The One that can be trusted. When I look at them I see someone who is probably not going to run a successful business again. They seem like they will never be. I think this may be because they have not had the chance to mature. They appear to have mostly faded as a result of a generation of talented people who were probably never meant to develop into successful businesses. I think these are the people who actually matter. They may be good people with great ideas, but not great prospects for success. They may be great people just waiting to be convinced they can turn things around. Maybe they are just boring, boring people who don't really care about anything beyond the silly ideas they entertain. They may be bright and engaging and would benefit from a lot of exposure if they got the chance. But they may not be very bright. The best way to tell this is that they are not really anyone to them. Really, really not anyone at all. A generation of brilliant people has mostly failed because of failures of these people's ideas. That's all well and good for them, but they don't really care about anything beyond their silly ideas. What bothers them is that they have all failed because they either don't care about anything beyond their silly ideas or they will continue to be brilliant because they do care about nothing but silly ideas. That's the worst waste of talent. A generation of brilliantly talented people does not really matter. They just don't care about anything beyond their silly ideas. That's the only waste of talent.\n",
      "A new comment:\n",
      "I would highly doubt that Jared Cowen is studying economics as an undergraduate. In other words, he's probably an undergraduate who thinks highly of various approaches to the same problem. Cowen could be an undergraduate who thinks highly of Michael Gerson or a student at the University of Chicago. Or he could be an undergraduate who thinks highly of John Maynard Keynes, whose insights were valuable but no less foolish in the 1950s. In any event, Ezra,\n",
      "\n",
      "[101 | 92.65] loss=1.07 avg=2.17\n",
      "[102 | 93.42] loss=0.86 avg=2.15\n",
      "[103 | 94.19] loss=1.11 avg=2.13\n",
      "[104 | 94.96] loss=1.14 avg=2.12\n",
      "[105 | 95.72] loss=1.01 avg=2.10\n",
      "[106 | 96.49] loss=0.95 avg=2.08\n",
      "[107 | 97.26] loss=1.10 avg=2.07\n",
      "[108 | 98.03] loss=0.99 avg=2.05\n",
      "[109 | 98.80] loss=0.89 avg=2.03\n",
      "[110 | 99.57] loss=1.00 avg=2.02\n",
      "[111 | 100.34] loss=0.64 avg=2.00\n",
      "[112 | 101.11] loss=0.71 avg=1.98\n",
      "[113 | 101.88] loss=0.71 avg=1.96\n",
      "[114 | 102.65] loss=0.92 avg=1.94\n",
      "[115 | 103.41] loss=1.04 avg=1.93\n",
      "[116 | 104.18] loss=0.87 avg=1.92\n",
      "[117 | 104.95] loss=0.76 avg=1.90\n",
      "[118 | 105.72] loss=0.75 avg=1.88\n",
      "[119 | 106.49] loss=0.58 avg=1.86\n",
      "[120 | 107.26] loss=0.95 avg=1.85\n",
      "[121 | 108.03] loss=0.74 avg=1.84\n",
      "[122 | 108.79] loss=0.79 avg=1.82\n",
      "[123 | 109.56] loss=0.75 avg=1.81\n",
      "[124 | 110.33] loss=0.67 avg=1.79\n",
      "[125 | 111.10] loss=0.49 avg=1.77\n",
      "[126 | 111.87] loss=0.52 avg=1.75\n",
      "[127 | 112.70] loss=0.42 avg=1.74\n",
      "[128 | 113.47] loss=0.65 avg=1.72\n",
      "[129 | 114.24] loss=0.64 avg=1.71\n",
      "[130 | 115.00] loss=0.58 avg=1.69\n",
      "[131 | 115.77] loss=0.60 avg=1.68\n",
      "[132 | 116.54] loss=0.81 avg=1.66\n",
      "[133 | 117.31] loss=0.71 avg=1.65\n",
      "[134 | 118.08] loss=0.48 avg=1.63\n",
      "[135 | 118.85] loss=0.65 avg=1.62\n",
      "[136 | 119.62] loss=0.55 avg=1.61\n",
      "[137 | 120.39] loss=0.44 avg=1.59\n",
      "[138 | 121.16] loss=0.43 avg=1.58\n",
      "[139 | 121.93] loss=0.46 avg=1.56\n",
      "[140 | 122.70] loss=0.38 avg=1.55\n",
      "[141 | 123.47] loss=0.59 avg=1.53\n",
      "[142 | 124.23] loss=0.31 avg=1.52\n",
      "[143 | 125.00] loss=0.73 avg=1.51\n",
      "[144 | 125.77] loss=0.29 avg=1.49\n",
      "[145 | 126.54] loss=0.77 avg=1.48\n",
      "[146 | 127.31] loss=0.39 avg=1.47\n",
      "[147 | 128.08] loss=0.70 avg=1.46\n",
      "[148 | 128.86] loss=0.34 avg=1.44\n",
      "[149 | 129.64] loss=0.28 avg=1.43\n",
      "[150 | 130.41] loss=0.60 avg=1.42\n",
      "[151 | 131.18] loss=0.34 avg=1.40\n",
      "[152 | 131.95] loss=0.27 avg=1.39\n",
      "[153 | 132.71] loss=0.33 avg=1.38\n",
      "[154 | 133.48] loss=0.31 avg=1.36\n",
      "[155 | 134.25] loss=0.40 avg=1.35\n",
      "[156 | 135.02] loss=0.30 avg=1.34\n",
      "[157 | 135.79] loss=0.29 avg=1.32\n",
      "[158 | 136.56] loss=0.25 avg=1.31\n",
      "[159 | 137.33] loss=0.38 avg=1.30\n",
      "[160 | 138.10] loss=0.34 avg=1.29\n",
      "[161 | 138.87] loss=0.31 avg=1.27\n",
      "[162 | 139.65] loss=0.24 avg=1.26\n",
      "[163 | 140.42] loss=0.22 avg=1.25\n",
      "[164 | 141.19] loss=0.25 avg=1.24\n",
      "[165 | 141.96] loss=0.16 avg=1.22\n",
      "[166 | 142.76] loss=0.26 avg=1.21\n",
      "[167 | 143.57] loss=0.27 avg=1.20\n",
      "[168 | 144.36] loss=0.28 avg=1.19\n",
      "[169 | 145.17] loss=0.17 avg=1.17\n",
      "[170 | 145.96] loss=0.24 avg=1.16\n",
      "[171 | 146.73] loss=0.21 avg=1.15\n",
      "[172 | 147.50] loss=0.17 avg=1.14\n",
      "[173 | 148.27] loss=0.35 avg=1.13\n",
      "[174 | 149.03] loss=0.31 avg=1.12\n",
      "[175 | 149.80] loss=0.19 avg=1.11\n",
      "[176 | 150.57] loss=0.21 avg=1.10\n",
      "[177 | 151.33] loss=0.21 avg=1.09\n",
      "[178 | 152.10] loss=0.24 avg=1.08\n",
      "[179 | 152.87] loss=0.21 avg=1.07\n",
      "[180 | 153.63] loss=0.20 avg=1.06\n",
      "[181 | 154.40] loss=0.19 avg=1.05\n",
      "[182 | 155.17] loss=0.18 avg=1.04\n",
      "[183 | 155.93] loss=0.23 avg=1.03\n",
      "[184 | 156.70] loss=0.17 avg=1.02\n",
      "[185 | 157.46] loss=0.15 avg=1.01\n",
      "[186 | 158.23] loss=0.16 avg=1.00\n",
      "[187 | 159.00] loss=0.14 avg=0.99\n",
      "[188 | 159.76] loss=0.17 avg=0.98\n",
      "[189 | 160.53] loss=0.17 avg=0.97\n",
      "[190 | 161.30] loss=0.14 avg=0.96\n",
      "[191 | 162.06] loss=0.17 avg=0.95\n",
      "[192 | 162.83] loss=0.15 avg=0.94\n",
      "[193 | 163.60] loss=0.17 avg=0.93\n",
      "[194 | 164.36] loss=0.16 avg=0.92\n",
      "[195 | 165.13] loss=0.13 avg=0.91\n",
      "[196 | 165.89] loss=0.13 avg=0.90\n",
      "[197 | 166.66] loss=0.24 avg=0.89\n",
      "[198 | 167.42] loss=0.10 avg=0.89\n",
      "[199 | 168.18] loss=0.18 avg=0.88\n",
      "[200 | 168.94] loss=0.15 avg=0.87\n",
      "======== SAMPLE 1 ========\n",
      " claims and a false claim that the Taliban does not have a \"programmatic denial strategy\" (DRB). It does note that \"[t]he goal of the Taliban' is not 'dramatically reducing the severity of its restrictions'. Rather, it is to reduce restrictions in a way that distributes power across different levels of government, which in turn distributes power across different levels of infrastructure.\" These changes will take time to implement, but it is clear from the comments that are excited by the idea of Dynaillamp.\n",
      "A new comment:\n",
      "@alex about syncing food into baskets so you don't have to chop veggies in. wihiyants thread popped out yesterday.\n",
      "A new comment:\n",
      "This appears to be a simple question. Food banks have spread to many sites, answering maybe 10%-20% of their inquiries. Most of the questions are self-explanatory, some self-explanatory. Self-explanatory? You know, like, % of the people who say the majority of their decisions [files vs. downloads] are motivated by consumption habits.\n",
      "A new comment:\n",
      "Interesting conversation. Perhaps the most interesting part is how (potentially inaccurately) the WSJ article relates to this feature. It seems to make a very direct link to https://www.worldwoke.org/2012/08/18/worldwide-tcp-study-world-health-tables-top-gotcha-us-programme-denials/. I wonder if the link is still there, if somebody took the time to run down all of the stuff that was obviously not covered by the authors bill, and grouped everything into categories, they wouldn't fall back on the WSJ draft link. If you are using a text based search engine such as “Karma” you probably don't. It takes a nifty little program such as “KarmaCalculator” and a few quotes, and replaces them with “text based” ones. So, if you are using that as my first try, it took me a few tries to get the link to show up there.\n",
      "A new comment:\n",
      "Great questions! Topics to keep an eye on: --Select- subject-- Arts and Sciences-- -AIScores-- -Coronary artery disease-- Bone marrow transplants-- Sudden cardiac deaths-- Liver failure-- Cardiovascular disease-- Lungs first? (Okay, but I Don't Like It When These Topics Are Uncovered By The Waitress, Credible Or Phrased It.) --Aesthetics/hygiene-- Fertility prevention-- Property taxes/consequences? Et cetera.\n",
      "A new comment:\n",
      "Thanks a lot, David! It's a pleasure to work with.\n",
      "A new comment:\n",
      "There is one Ra here who is still convinced oral sex is a thing. She tells jokes about it being a joke and then he pats her on the back.\n",
      "A new comment:\n",
      "\"It is therefore not an issue of concern\" To learn more about Courtney, sex ed, drone abuse, guns, guns, foreign aid, foreign policy, and more, check out this: https://www.washingtontimes.com/list/sydney?_w=980&_t=AAUSklQzaJZJzaMT=1 Another link: https://news.gallup.com/poll/389358/us-cis-dodging-trump-interviews-covid-writer-covid-story/\n",
      "A new comment:\n",
      "#4. It is a gross overstatement to say that the anti-choicers in the party don't really care what people call their own territory. That may be true in some respects, but for general elections it is critical to have a large section of the base who is open to changing of anything. If you know you might get something wrong, you want to hear from people who have done exactly that, regardless of what the other camp says.\n",
      "A new comment:\n",
      "4. Before long all of this is over. The anti-choicers have decided to call the shots in a more egalitarian country like India, where women are far less likely to get booked into the backwards model and still face hurdles in getting promotions. That may be sustainable, but it isn't certain certainty. The military in India, for instance, has had gender pay decisions passed that are in some ways hostile to reality. And even if the gender pay decisions of the typical man are not as widely implemented as in India, they are still a big deal there. This may be sustainable as a means to an end, but it is also true that the women around me don't seem to be that different from you. As for the murder case, that is another story. As for the sex abuse accusations, that don't give a whole damn about it. They are just that women made the accusations.\n",
      "\n",
      "\n",
      "[201 | 176.60] loss=0.13 avg=0.86\n",
      "[202 | 177.36] loss=0.12 avg=0.85\n",
      "[203 | 178.12] loss=0.11 avg=0.84\n",
      "[204 | 178.88] loss=0.14 avg=0.84\n",
      "[205 | 179.64] loss=0.10 avg=0.83\n",
      "[206 | 180.40] loss=0.09 avg=0.82\n",
      "[207 | 181.16] loss=0.14 avg=0.81\n",
      "[208 | 181.92] loss=0.10 avg=0.80\n",
      "[209 | 182.68] loss=0.13 avg=0.79\n",
      "[210 | 183.44] loss=0.10 avg=0.79\n",
      "[211 | 184.20] loss=0.11 avg=0.78\n",
      "[212 | 184.96] loss=0.13 avg=0.77\n",
      "[213 | 185.72] loss=0.09 avg=0.76\n",
      "[214 | 186.48] loss=0.09 avg=0.76\n",
      "[215 | 187.25] loss=0.14 avg=0.75\n",
      "[216 | 188.01] loss=0.12 avg=0.74\n",
      "[217 | 188.77] loss=0.09 avg=0.74\n",
      "[218 | 189.53] loss=0.10 avg=0.73\n",
      "[219 | 190.29] loss=0.10 avg=0.72\n",
      "[220 | 191.05] loss=0.17 avg=0.71\n",
      "[221 | 191.81] loss=0.10 avg=0.71\n",
      "[222 | 192.57] loss=0.09 avg=0.70\n",
      "[223 | 193.33] loss=0.11 avg=0.69\n",
      "[224 | 194.09] loss=0.19 avg=0.69\n",
      "[225 | 194.85] loss=0.12 avg=0.68\n",
      "[226 | 195.61] loss=0.10 avg=0.68\n",
      "[227 | 196.38] loss=0.23 avg=0.67\n",
      "[228 | 197.14] loss=0.11 avg=0.66\n",
      "[229 | 197.90] loss=0.14 avg=0.66\n",
      "[230 | 198.66] loss=0.09 avg=0.65\n",
      "[231 | 199.42] loss=0.20 avg=0.65\n",
      "[232 | 200.18] loss=0.10 avg=0.64\n",
      "[233 | 200.94] loss=0.11 avg=0.64\n",
      "[234 | 201.70] loss=0.12 avg=0.63\n",
      "[235 | 202.48] loss=0.09 avg=0.62\n",
      "[236 | 203.24] loss=0.10 avg=0.62\n",
      "[237 | 204.00] loss=0.08 avg=0.61\n",
      "[238 | 204.76] loss=0.06 avg=0.61\n",
      "[239 | 205.52] loss=0.08 avg=0.60\n",
      "[240 | 206.28] loss=0.11 avg=0.59\n",
      "[241 | 207.05] loss=0.10 avg=0.59\n",
      "[242 | 207.81] loss=0.08 avg=0.58\n",
      "[243 | 208.57] loss=0.13 avg=0.58\n",
      "[244 | 209.33] loss=0.11 avg=0.57\n",
      "[245 | 210.09] loss=0.12 avg=0.57\n",
      "[246 | 210.85] loss=0.09 avg=0.56\n",
      "[247 | 211.61] loss=0.09 avg=0.56\n",
      "[248 | 212.37] loss=0.10 avg=0.55\n",
      "[249 | 213.13] loss=0.18 avg=0.55\n",
      "[250 | 213.89] loss=0.06 avg=0.54\n",
      "[251 | 214.65] loss=0.08 avg=0.54\n",
      "[252 | 215.41] loss=0.08 avg=0.53\n",
      "[253 | 216.17] loss=0.09 avg=0.53\n",
      "[254 | 216.93] loss=0.09 avg=0.52\n",
      "[255 | 217.70] loss=0.09 avg=0.52\n",
      "[256 | 218.46] loss=0.10 avg=0.51\n",
      "[257 | 219.22] loss=0.08 avg=0.51\n",
      "[258 | 219.98] loss=0.09 avg=0.51\n",
      "[259 | 220.74] loss=0.09 avg=0.50\n",
      "[260 | 221.50] loss=0.08 avg=0.50\n",
      "[261 | 222.26] loss=0.13 avg=0.49\n",
      "[262 | 223.02] loss=0.09 avg=0.49\n",
      "[263 | 223.78] loss=0.11 avg=0.48\n",
      "[264 | 224.54] loss=0.05 avg=0.48\n",
      "[265 | 225.30] loss=0.08 avg=0.48\n",
      "[266 | 226.06] loss=0.11 avg=0.47\n",
      "[267 | 226.83] loss=0.09 avg=0.47\n",
      "[268 | 227.59] loss=0.41 avg=0.47\n",
      "[269 | 228.35] loss=0.09 avg=0.46\n",
      "[270 | 229.11] loss=0.08 avg=0.46\n",
      "[271 | 229.87] loss=0.08 avg=0.45\n",
      "[272 | 230.63] loss=0.10 avg=0.45\n",
      "[273 | 231.39] loss=0.07 avg=0.45\n",
      "[274 | 232.15] loss=0.07 avg=0.44\n",
      "[275 | 232.94] loss=0.10 avg=0.44\n",
      "[276 | 233.70] loss=0.07 avg=0.44\n",
      "[277 | 234.46] loss=0.07 avg=0.43\n",
      "[278 | 235.22] loss=0.07 avg=0.43\n",
      "[279 | 235.98] loss=0.08 avg=0.42\n",
      "[280 | 236.74] loss=0.08 avg=0.42\n",
      "[281 | 237.50] loss=0.07 avg=0.42\n",
      "[282 | 238.26] loss=0.09 avg=0.41\n",
      "[283 | 239.02] loss=0.07 avg=0.41\n",
      "[284 | 239.79] loss=0.07 avg=0.41\n",
      "[285 | 240.55] loss=0.09 avg=0.40\n",
      "[286 | 241.31] loss=0.09 avg=0.40\n",
      "[287 | 242.07] loss=0.06 avg=0.40\n",
      "[288 | 242.83] loss=0.11 avg=0.39\n",
      "[289 | 243.59] loss=0.09 avg=0.39\n",
      "[290 | 244.35] loss=0.09 avg=0.39\n",
      "[291 | 245.11] loss=0.08 avg=0.38\n",
      "[292 | 245.87] loss=0.08 avg=0.38\n",
      "[293 | 246.63] loss=0.08 avg=0.38\n",
      "[294 | 247.39] loss=0.07 avg=0.37\n",
      "[295 | 248.15] loss=0.07 avg=0.37\n",
      "[296 | 248.92] loss=0.09 avg=0.37\n",
      "[297 | 249.69] loss=0.07 avg=0.36\n",
      "[298 | 250.45] loss=0.04 avg=0.36\n",
      "[299 | 251.21] loss=0.06 avg=0.36\n",
      "[300 | 251.98] loss=0.08 avg=0.35\n",
      "Saving checkpoint/run1/model-300\n",
      "https://www.reddit.com/r/The_Donald/comments/5qd7xu/late_night_crew_we_played_a_mask_on_the_UC_not_for_exactly_the_same_behavior/\n",
      "A new comment:\n",
      "#DonaldTrump, the renegotiated globalist dream is dead.\n",
      "A new comment:\n",
      "\"The stars align and the stars align again.\" Well, this is still a very good idea, if not, then at least a few dozen stars will align and the stars will adjust. If we think of the earth as flat we mean it all. We should also remember that the central planners of the 19th century were not people who sat around a fire arguing over how to divide the food around. They were farmers who packed their produce into small pots and set them on poles, ate them, and moved to make vegetable oil to replace the potatoes, and made nigella oil to replace the peanuts. The oil was too heavy for the potatoes and the peanuts too heavy for the rooster. The furor led to the Salem witch trials.\n",
      "A new comment:\n",
      "\"lest our awareness of this pandemic turn to the talent for rational discussion and debate paralysis, we must consider this momentous decision “we’s only chance of winning this race”\": A good starting point. It is not. Talent exists only when the contestants have not yet been properly trained to see it. When the elites have not yet been properly trained to see it, then a surge in talent by any measure is inevitable. This is why the anti-immigrant crowd has been running wild with “we’s”—and why the Super Bowl is about to be played in California. Talent hovers around the 15-20% mark for some, but the rest of the talent hovers around 10-15% for others, and 10-15% for the rich and powerful like myself.\n",
      "A new comment:\n",
      "#7 Are Tesla Model S and Model Xs going to be niche? Cars are niche because of the convenience of shared mobility, but the travel markets also have demand for niche products and for niche markets are niche because of it. The EBITDA from niche mobility is huge. My local drugstore got 6 million units sold in the first week, and now has 10 million units sold in the next week. If you are a patient consumer and want to let that patient mobility be managed by a specialist, you are good to go. If not, you can always sell products through your local pharma dealer.\n",
      "A new comment:\n",
      "On C-Suite error, running the timer incorrectly might lead to cancer. I just want people to be very clear about this. --Ed.\n",
      "A new comment:\n",
      "\"7. The folks who run our country's schools don't seem to be very happy with how things are going.\" Is there a \"path\" to happiness that doesn't set the danger level at orgy-like for teachers? The unions' war-rage over raises, raises, raises--this is the future. We can't let the threats of nuclear war stop us from moving to a safer environment.\n",
      "A new comment:\n",
      "The fault is not with the teachers. The teachers' healthcare debacle is.\n",
      "A new comment:\n",
      "4. Little Tyler forgot to link to this great title from his WPThis is a reference for the great many WPThis essays. Good ones by Alex, David and Ross too.\n",
      "A new comment:\n",
      "1. Yes, time to rename the robo-registration system. The changes were made to help give the most qualified candidates entry into the system, not to mention educational. The reason for doing this is that more people would mean more slots for higher education institutions to advertise and promote. This may seem counterintuitive, but institutions are now more selective in how they advertise and promote, so it works for all kinds of reasons. The change to \"new train every seven years\" is also selective. There are already more than 800 new schools in England and Wales that are members of the New Train EverySevenSites.us, which is a lot of schools being selective in how they promote and advertise. It's also important to note that the change is not in schools, but rather in the regents, who have pronounced in favor of promoting education that emphasizes the academic and cultural traditions. This may seem counterintuitive, but institutions are now more selective in how they promote and promote, so it works for all kinds of reasons. The change to \"new train every seven years\" is also selective. There are already more than 800 schools in England and Wales that are member of the New Train EverySevenSites.us, which is a lot of schools being selective in how they promote and promote. It's also important to note that the change is not in schools, but rather in the bishops. The organization is clear about this. When I went to school as a boy, The Quaker\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              'dataset.txt',\n",
    "              model_name=model_name,\n",
    "              steps=300)#1000)   # steps is max number of training steps\n",
    "\n",
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0ef9b-6774-4b23-8b08-6159aee62b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow GPU 2.6 (py39)",
   "language": "python",
   "name": "tensorflow-gpu-2.6-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
